{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification avec un neurone - Descente de gradient\n",
    "\n",
    "Nous souhaitons pouvoir dire pour chaque point d'un plan s’il devrait être colorié en rouge ou bien en bleu à partir d'informations issues d'un jeu de données initial. Pour cela on décide de construire le réseau le plus simple possible : avec un seul neurone. Cela correspond donc à séparer les points du plan selon une droite. La dimension de\n",
    "l’entrée est 2, les coordonnées (x, y) de chaque point du jeux initial, et celle la sortie est 1, point rouge ou bleu. Nous utiliserons la fonction d'activation sigmoide. Il y faut déterminer les paramètres, correspondant aux poids et au biais, qui permettent de trouver cette droite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage\n",
    "Pour trouver les paramètres du neurone qui définissent la meilleure fonction F permettant la classification des données, il s’agit de minimiser l’erreur E (comme vu en cours). Pour cela on utilise la méthode de la descente de gradient. Pour cela il vous faudra trouver puis implémenter les équations associées. \n",
    "\n",
    "Nous allons utiliser python et les packages de base ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir les différentes fonctions qui nous permettront de résoudre notre problème.\n",
    "\n",
    "### Fonction d'activation\n",
    "Définir la fonction sigmoïde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    \n",
    "    return \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'erreur\n",
    "Déterminer l’erreur locale pour la donnée numéro $i$ et l'erreur globale. Construire la fonction associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(w1, w2, b):\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des gradients\n",
    "Déterminer les expressions des différents gradients et construire les fonctions associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_grad(w1, w2, b, y, data):\n",
    "    gw1 = \n",
    "    gw2 = \n",
    "    gb = \n",
    "\n",
    "    return gw1, gw2, gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_E(w1, w2, b):\n",
    "    gw1_red, gw2_red, gb_red = calcul_grad(w1, w2, b, 1, carres_rouges)\n",
    "    gw1_blu, gw2_blu, gb_blu = calcul_grad(w1, w2, b, 0, ronds_bleus)\n",
    "    \n",
    "    return np.array([ ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme de descente de gradient\n",
    "Construire l'algorithme de descente de gradient qui fera appel aux fonctions précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente(theta0, delta=0.1, nmax=10):\n",
    "    liste_theta = [theta0] # sauvegarde valeurs successive de X\n",
    "    liste_grad = [] # sauvegarde valeurs successives du gradient\n",
    "    theta = theta0\n",
    "    for i in range(nmax):\n",
    "        gradient = \n",
    "        theta = \n",
    "        liste_theta.append(theta)\n",
    "        liste_grad.append(gradient)\n",
    "   \n",
    "    return liste_theta, liste_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions d'affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiche_descente(f, liste_theta, liste_grad, delta, nmax=10):\n",
    "    print(\"vittesse d'apprentissage : \",delta)\n",
    "    print(\"Nombre d'itérations :\", nmax)\n",
    "    print(\"Paramètres initiaux :\", theta0)\n",
    "    for i in range(len(liste_theta)-1):\n",
    "        print(\"--- Etape\",i)\n",
    "        print(\"Paramètres :\", *liste_theta[i])\n",
    "        print(\"Gradient \", *liste_grad[i])\n",
    "        print(\"Valeur de la fonction coût \", f(*liste_theta[i]))\n",
    "    print(\"--- --------------------\")\n",
    "    print(\"Dernier point :\", *liste_theta[-1])\n",
    "    print(\"Dernière valeur de la fonction coût \", f(*liste_theta[-1]))\n",
    "    print(\"--- --------------------\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphique_points(w1, w2, b):\n",
    "    for x, y in carres_rouges:    \n",
    "        plt.scatter(x, y, marker='s', color='red')\n",
    "    for x, y in ronds_bleus:   \n",
    "        plt.scatter(x, y, color='blue')\n",
    "\n",
    "    VX = np.linspace(-0.5, 5.5, 100)\n",
    "    VY = -1/w2*(w1*VX+b)\n",
    "\n",
    "    #  affichage\n",
    "    plt.plot(VX, VY, color='black')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-0.5,5.5)\n",
    "    plt.ylim(-0.5,5.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en oeuvre\n",
    "Vous pouvez maintenant tester votre solution à partir de l'exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Génération des données initiales : carré rouge / rond bleu \n",
    "carres_rouges = [(1,1), (2,0.5), (2,2), (3,1.5), (3,2.75), (4,1), (4,2.5), (4.5,3), (5,1), (5,2.25)]\n",
    "ronds_bleus   = [(0,3), (1,1.5), (1,4), (1.5,2.5), (2,2.5), (3,3.5), (3.5,3.25), (4,3), (4,4), (5,4)] \n",
    "\n",
    "N = len(carres_rouges) + len(ronds_bleus)    # taille des données\n",
    " \n",
    "# initialisation des paramètres du neurone\n",
    "theta0 = np.array([0, 1, -2])\n",
    "\n",
    "# recherche des paramètres du neurone\n",
    "mon_delta = 1\n",
    "mon_nmax = 10\n",
    "liste_theta, liste_grad = descente(grad_E, theta0, delta=mon_delta, nmax=mon_nmax)\n",
    "\n",
    "affiche_descente(E, liste_theta, liste_grad, delta=mon_delta, nmax=mon_nmax)\n",
    "\n",
    "w1, w2, b = liste_theta[-1]\n",
    "print(\"Coefficients w1, w2, b du neurone et de la droite de séparation :\", round(w1,5), round(w2,5), round(b,5))\n",
    "\n",
    "graphique_points(w1, w2, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commenter les résultats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction\n",
    "Vous avez obtenu par la descente de gradient les paramètres du neurone qui permettent de séparer le plan en deux parties. Il est maintenat possible de l'utiliser pour prédire à quelle classe appartient n'importe quel point de l'espace.\n",
    "\n",
    "Prédire par exemple, la classe des points de coordonnées (x1=2, x2=3) et de (x1=2, x2=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1,w2,b,x1,x2):\n",
    "    F=sigmoide(w1*x1+w2*x2+b)\n",
    "    if F > 0.5:\n",
    "        print('rouge', F)\n",
    "        graphique_new_points(x1,x2, 1, w1, w2, b)\n",
    "    else:\n",
    "        print('bleu', F)\n",
    "        graphique_new_points(x1,x2, 0, w1, w2, b)\n",
    "    return\n",
    "\n",
    "predict()\n",
    "predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commenter les résultats\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
